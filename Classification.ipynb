{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcc82f48-aaeb-45c8-b844-9d42411208a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels vector saved with 10000 entries.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load the JSON file\n",
    "json_file = \"LLMgenerated_outputs.json\"  # Replace with your actual file path\n",
    "with open(json_file, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "    \n",
    "labels = []\n",
    "for image_path, output in data.items():  # Iterate over key-value pairs\n",
    "    if \"notdepressed\" in image_path.lower():\n",
    "        labels.append(0)\n",
    "    elif \"depressed\" in image_path.lower():\n",
    "        labels.append(1)\n",
    "\n",
    "# Save labels to a NumPy file\n",
    "labels = np.array(labels)\n",
    "np.save(\"labels.npy\", labels)\n",
    "print(f\"Labels vector saved with {len(labels)} entries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836e8aad-6075-4eab-96e7-0c0a88029b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62fadff0-7340-4e0c-8b7c-224d0d73fdd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sgn4hf/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text embeddings saved for 10000 items.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Load JSON file\n",
    "\n",
    "# Set up device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "\n",
    "def get_text_embeddings(caption):\n",
    "    \"\"\"\n",
    "    Generate BERT embeddings for a given caption.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(caption, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.cpu().numpy()\n",
    "\n",
    "# Generate text embeddings for all captions in JSON data\n",
    "text_embeddings = {}\n",
    "for image_path, caption in data.items():  # Iterate over key-value pairs\n",
    "    try:\n",
    "        embedding = get_text_embeddings(caption)\n",
    "        text_embeddings[image_path] = embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "# Save text embeddings as a NumPy file\n",
    "np.save(\"llm_text_embeddings_10k.npy\", text_embeddings)\n",
    "print(f\"Text embeddings saved for {len(text_embeddings)} items.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18bf253-cf5e-4e33-a893-99d7e4166406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d89f2b37-3baa-47c6-89e1-b202a577dcaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (10000, 2304)\n",
      "Labels shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Load embeddings and labels\n",
    "text_embeddings_file = \"text_embeddings.npy\"  # BERT-generated text embeddings\n",
    "image_embeddings_file = \"image_embeddings.npy\"  # Image embeddings\n",
    "llm_text_embeddings_file = \"llm_text_embeddings_10k.npy\"  # LLM-generated text embeddings\n",
    "labels_file = \"labels.npy\"  # Labels\n",
    "\n",
    "# Load data\n",
    "text_embeddings = np.load(text_embeddings_file, allow_pickle=True).item()\n",
    "image_embeddings = np.load(image_embeddings_file, allow_pickle=True).item()\n",
    "llm_text_embeddings = np.load(llm_text_embeddings_file, allow_pickle=True).item()\n",
    "labels = np.load(labels_file)\n",
    "\n",
    "# Load JSON to get the image paths\n",
    "json_file = \"LLMgenerated_outputs.json\"  # Replace with your actual file path\n",
    "\n",
    "with open(json_file, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize feature matrix and labels\n",
    "feature_matrix = []\n",
    "final_labels = []\n",
    "\n",
    "# Iterate over all data and concatenate embeddings\n",
    "for idx, (image_path, llm_caption) in enumerate(data.items()):\n",
    "    if image_path in text_embeddings and image_path in image_embeddings and image_path in llm_text_embeddings:\n",
    "        # Concatenate all embeddings into a single vector\n",
    "        combined_vector = np.concatenate((\n",
    "            text_embeddings[image_path].flatten(),  # Flatten BERT text embedding\n",
    "            image_embeddings[image_path].flatten(),  # Flatten image embedding\n",
    "            llm_text_embeddings[image_path].flatten()  # Flatten LLM text embedding\n",
    "        ))\n",
    "        feature_matrix.append(combined_vector)\n",
    "        final_labels.append(labels[idx])  # Ensure labels are aligned with the features\n",
    "\n",
    "# Convert feature matrix and labels to NumPy arrays\n",
    "feature_matrix = np.array(feature_matrix)\n",
    "final_labels = np.array(final_labels)\n",
    "\n",
    "# Save the feature matrix and labels for future use\n",
    "np.save(\"feature_matrix.npy\", feature_matrix)\n",
    "np.save(\"final_labels.npy\", final_labels)\n",
    "\n",
    "print(f\"Feature matrix shape: {feature_matrix.shape}\")\n",
    "print(f\"Labels shape: {final_labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de2d8291-5864-41b4-b3cb-c55650154ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting xgboost\n",
      "  Obtaining dependency information for xgboost from https://files.pythonhosted.org/packages/32/93/66826e2f50cefecbb0a44bd1e667316bf0a3c8e78cd1f0cdf52f5b2c5c6f/xgboost-2.1.3-py3-none-manylinux_2_28_x86_64.whl.metadata\n",
      "  Downloading xgboost-2.1.3-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in /apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages (from xgboost) (1.24.4)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /home/sgn4hf/.local/lib/python3.11/site-packages (from xgboost) (2.20.5)\n",
      "Requirement already satisfied: scipy in /apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages (from xgboost) (1.11.2)\n",
      "Downloading xgboost-2.1.3-py3-none-manylinux_2_28_x86_64.whl (153.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.9/153.9 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.3\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b626777-4897-4ff1-99a2-b85bedc7c5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Logistic Regression:\n",
      "  Accuracy: 0.578\n",
      "  Precision: 0.47619047619047616\n",
      "  Recall: 0.44554455445544555\n",
      "  F1 Score: 0.46035805626598464\n",
      "  ROC AUC: 0.5735498413515849\n",
      "  Confusion Matrix: [[796 396]\n",
      " [448 360]]\n",
      "\n",
      "Training Random Forest...\n",
      "Results for Random Forest:\n",
      "  Accuracy: 0.617\n",
      "  Precision: 0.5921052631578947\n",
      "  Recall: 0.1670792079207921\n",
      "  F1 Score: 0.26061776061776065\n",
      "  ROC AUC: 0.5987425451857266\n",
      "  Confusion Matrix: [[1099   93]\n",
      " [ 673  135]]\n",
      "\n",
      "Training Support Vector Machine (SVM)...\n",
      "Results for Support Vector Machine (SVM):\n",
      "  Accuracy: 0.6275\n",
      "  Precision: 0.6046511627906976\n",
      "  Recall: 0.22524752475247525\n",
      "  F1 Score: 0.32822362488728585\n",
      "  ROC AUC: 0.6351257766296763\n",
      "  Confusion Matrix: [[1073  119]\n",
      " [ 626  182]]\n",
      "\n",
      "Training K-Nearest Neighbors (KNN)...\n",
      "Results for K-Nearest Neighbors (KNN):\n",
      "  Accuracy: 0.5865\n",
      "  Precision: 0.48671328671328673\n",
      "  Recall: 0.4306930693069307\n",
      "  F1 Score: 0.45699277741300065\n",
      "  ROC AUC: 0.5891218893281945\n",
      "  Confusion Matrix: [[825 367]\n",
      " [460 348]]\n",
      "\n",
      "Training Decision Tree...\n",
      "Results for Decision Tree:\n",
      "  Accuracy: 0.531\n",
      "  Precision: 0.4222488038277512\n",
      "  Recall: 0.4368811881188119\n",
      "  F1 Score: 0.42944038929440387\n",
      "  ROC AUC: 0.5134404694664096\n",
      "  Confusion Matrix: [[709 483]\n",
      " [455 353]]\n",
      "\n",
      "Training Gradient Boosting...\n",
      "Results for Gradient Boosting:\n",
      "  Accuracy: 0.612\n",
      "  Precision: 0.5536912751677853\n",
      "  Recall: 0.2042079207920792\n",
      "  F1 Score: 0.298372513562387\n",
      "  ROC AUC: 0.5962548383613528\n",
      "  Confusion Matrix: [[1059  133]\n",
      " [ 643  165]]\n",
      "\n",
      "Training XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sgn4hf/.local/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [19:26:39] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for XGBoost:\n",
      "  Accuracy: 0.6005\n",
      "  Precision: 0.5080500894454383\n",
      "  Recall: 0.35148514851485146\n",
      "  F1 Score: 0.415508412582297\n",
      "  ROC AUC: 0.6041997184198284\n",
      "  Confusion Matrix: [[917 275]\n",
      " [524 284]]\n",
      "\n",
      "Summary of Classifier Performance:\n",
      "Logistic Regression: Accuracy = 0.58, F1 Score = 0.46, ROC AUC = 0.5735498413515849\n",
      "Random Forest: Accuracy = 0.62, F1 Score = 0.26, ROC AUC = 0.5987425451857266\n",
      "Support Vector Machine (SVM): Accuracy = 0.63, F1 Score = 0.33, ROC AUC = 0.6351257766296763\n",
      "K-Nearest Neighbors (KNN): Accuracy = 0.59, F1 Score = 0.46, ROC AUC = 0.5891218893281945\n",
      "Decision Tree: Accuracy = 0.53, F1 Score = 0.43, ROC AUC = 0.5134404694664096\n",
      "Gradient Boosting: Accuracy = 0.61, F1 Score = 0.30, ROC AUC = 0.5962548383613528\n",
      "XGBoost: Accuracy = 0.60, F1 Score = 0.42, ROC AUC = 0.6041997184198284\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load feature matrix and labels\n",
    "feature_matrix = np.load(\"feature_matrix.npy\")\n",
    "final_labels = np.load(\"final_labels.npy\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    feature_matrix, final_labels, test_size=0.2, random_state=42, stratify=final_labels\n",
    ")\n",
    "\n",
    "# Standardize the feature matrix\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Function to evaluate classifiers\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train the model and evaluate it on the test set.\n",
    "    \"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"F1 Score\": f1_score(y_test, y_pred),\n",
    "        \"ROC AUC\": roc_auc_score(y_test, y_prob) if y_prob is not None else \"N/A\",\n",
    "        \"Confusion Matrix\": confusion_matrix(y_test, y_pred),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# List of classifiers\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Support Vector Machine (SVM)\": SVC(probability=True),\n",
    "    \"K-Nearest Neighbors (KNN)\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "results = {}\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    metrics = evaluate_model(clf, X_train, X_test, y_train, y_test)\n",
    "    results[name] = metrics\n",
    "    print(f\"Results for {name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value}\")\n",
    "    print()\n",
    "\n",
    "# Print a summary of all results\n",
    "print(\"Summary of Classifier Performance:\")\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name}: Accuracy = {metrics['Accuracy']:.2f}, F1 Score = {metrics['F1 Score']:.2f}, ROC AUC = {metrics['ROC AUC']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e013372-9d73-43e8-9aa0-76367ef7a6c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a623e9fb-8999-4718-9bd4-34ff25728263",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "# Load feature matrix and labels\n",
    "feature_matrix = np.load(\"feature_matrix.npy\")\n",
    "final_labels = np.load(\"final_labels.npy\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    feature_matrix, final_labels, test_size=0.2, random_state=42, stratify=final_labels\n",
    ")\n",
    "\n",
    "# Standardize the feature matrix\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c14dcbe-b326-477c-9016-6052e6dc6a50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "196b33f0-b7da-4535-bb85-46ec9fc208c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Best Parameters for KNN: {'metric': 'euclidean', 'n_neighbors': 5, 'weights': 'distance'}\n",
      "Best F1 Score for KNN: 0.45387862730444517\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define hyperparameter grid\n",
    "knn_param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search_knn = GridSearchCV(knn, knn_param_grid, cv=5, scoring='f1', verbose=1, n_jobs=-1)\n",
    "grid_search_knn.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(f\"Best Parameters for KNN: {grid_search_knn.best_params_}\")\n",
    "print(f\"Best F1 Score for KNN: {grid_search_knn.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8e28dc-f7ba-4234-aebb-a90376099298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d31b23e2-e525-4087-8efa-ab092f940746",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for Logistic Regression: {'C': 0.01, 'solver': 'liblinear'}\n",
      "Best F1 Score for Logistic Regression: 0.462310016807717\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define hyperparameter grid\n",
    "lr_param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "logistic_regression = LogisticRegression(max_iter=500)\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search_lr = GridSearchCV(logistic_regression, lr_param_grid, cv=5, scoring='f1', verbose=1, n_jobs=-1)\n",
    "grid_search_lr.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(f\"Best Parameters for Logistic Regression: {grid_search_lr.best_params_}\")\n",
    "print(f\"Best F1 Score for Logistic Regression: {grid_search_lr.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cfb3bf-7dbc-4d41-8cc5-23050dd16616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea7455d5-c649-4dfa-b9cd-23dc751624bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best Parameters for Decision Tree: {'criterion': 'gini', 'max_depth': None, 'max_features': None}\n",
      "Best F1 Score for Decision Tree: 0.4498924334931441\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define hyperparameter grid\n",
    "dt_param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],             # Split quality measure\n",
    "    'max_depth': [None, 10, 20, 30],              # Depth of the tree\n",
    "    'max_features': [None, 'sqrt', 'log2'],       # Max features for a split\n",
    "}\n",
    "\n",
    "# Initialize Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search_dt = GridSearchCV(\n",
    "    dt, dt_param_grid, cv=5, scoring='f1', verbose=1, n_jobs=-1\n",
    ")\n",
    "grid_search_dt.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best F1 score\n",
    "print(f\"Best Parameters for Decision Tree: {grid_search_dt.best_params_}\")\n",
    "print(f\"Best F1 Score for Decision Tree: {grid_search_dt.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914af344-54fd-44fd-afb2-3903939f2a39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea02c0-d6c2-473e-a333-74be03e92399",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
